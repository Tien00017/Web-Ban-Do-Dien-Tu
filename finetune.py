import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import open_clip
import pandas as pd
from PIL import Image
from tqdm import tqdm
import os

# --- C·∫§U H√åNH ---
CSV_FILE = 'pc_parts_captions_blip.csv'  # File CSV b·∫°n ƒë√£ t·∫°o
MODEL_NAME = 'RN50'                      # D√πng ResNet50
PRETRAINED = 'openai'
BATCH_SIZE = 16                          # Gi·∫£m xu·ªëng n·∫øu tr√†n VRAM (Out of Memory)
EPOCHS = 5                               # S·ªë l·∫ßn h·ªçc l·∫∑p l·∫°i to√†n b·ªô dataset
LEARNING_RATE = 1e-5                     # T·ªëc ƒë·ªô h·ªçc th·∫•p ƒë·ªÉ kh√¥ng l√†m h·ªèng ki·∫øn th·ª©c c≈©
SAVE_PATH = 'finetuned_clip.pt'          # N∆°i l∆∞u model sau khi train
# ----------------

class PCPartsDataset(Dataset):
    def __init__(self, csv_file, preprocess, tokenizer):
        self.data = pd.read_csv(csv_file)
        self.preprocess = preprocess
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        image_path = row['image_path_full']
        caption = str(row['caption_auto'])

        # X·ª≠ l√Ω ·∫£nh
        try:
            image = Image.open(image_path).convert("RGB")
            image_tensor = self.preprocess(image)
        except Exception:
            # N·∫øu l·ªói ·∫£nh, t·∫°o ·∫£nh ƒëen ƒë·ªÉ kh√¥ng crash (c√°ch x·ª≠ l√Ω t·∫°m th·ªùi)
            image_tensor = torch.zeros((3, 224, 224))
        
        # X·ª≠ l√Ω vƒÉn b·∫£n (Tokenize)
        text_tokens = self.tokenizer([caption])[0]  # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n

        return image_tensor, text_tokens

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üöÄ B·∫Øt ƒë·∫ßu Finetuning tr√™n thi·∫øt b·ªã: {device}")

    # 1. T·∫£i Model v√† c√°c c√¥ng c·ª• h·ªó tr·ª£
    model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED, device=device)
    tokenizer = open_clip.get_tokenizer(MODEL_NAME)

    # N·∫øu mu·ªën finetune to√†n b·ªô, ta kh√¥ng ƒë√≥ng bƒÉng (freeze) l·ªõp n√†o c·∫£.
    # Nh∆∞ng ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ, ƒë√¥i khi ng∆∞·ªùi ta kh√≥a Image Encoder, ch·ªâ train Text Encoder ho·∫∑c ng∆∞·ª£c l·∫°i.
    # ·ªû ƒë√¢y ta train c·∫£ hai (Full Finetuning).

    # 2. Chu·∫©n b·ªã D·ªØ li·ªáu
    if not os.path.exists(CSV_FILE):
        print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV. H√£y ch·∫°y main.py tr∆∞·ªõc ƒë·ªÉ t·∫°o d·ªØ li·ªáu."); return

    dataset = PCPartsDataset(CSV_FILE, preprocess, tokenizer)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

    # 3. Thi·∫øt l·∫≠p Optimizer v√† Loss
    loss_img = nn.CrossEntropyLoss()
    loss_txt = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)

    # 4. V√≤ng l·∫∑p Hu·∫•n luy·ªán (Training Loop)
    for epoch in range(EPOCHS):
        model.train() # Chuy·ªÉn sang ch·∫ø ƒë·ªô train
        total_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")

        for batch in pbar:
            optimizer.zero_grad()
            
            images, texts = batch
            images = images.to(device)
            texts = texts.to(device)

            # Forward pass (T√≠nh to√°n)
            # Model tr·∫£ v·ªÅ ƒë·∫∑c tr∆∞ng ·∫£nh v√† vƒÉn b·∫£n, c√πng v·ªõi logit_scale
            image_features, text_features, logit_scale = model(images, texts)
            
            # T√≠nh to√°n ma tr·∫≠n t∆∞∆°ng ƒë·ªìng (Similarity Matrix)
            logit_scale = logit_scale.exp()
            logits_per_image = logit_scale * image_features @ text_features.t()
            logits_per_text = logits_per_image.t()

            # T·∫°o nh√£n (Labels): ƒê∆∞·ªùng ch√©o c·ªßa ma tr·∫≠n l√† c·∫∑p ƒë√∫ng (0, 1, 2...)
            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)

            # T√≠nh Loss t·ªïng h·ª£p (Symmetric Loss)
            loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2

            # Backward pass (C·∫≠p nh·∫≠t tr·ªçng s·ªë)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            pbar.set_postfix({"Loss": f"{loss.item():.4f}"})

        avg_loss = total_loss / len(dataloader)
        print(f"‚úÖ K·∫øt th√∫c Epoch {epoch+1}, Loss trung b√¨nh: {avg_loss:.4f}")

    # 5. L∆∞u Model
    print(f"üíæ ƒêang l∆∞u model v√†o {SAVE_PATH}...")
    torch.save(model.state_dict(), SAVE_PATH)
    print("üéâ Finetuning ho√†n t·∫•t!")

if __name__ == "__main__":
    main()